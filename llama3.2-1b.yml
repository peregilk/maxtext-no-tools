base_emb_dim: 2048
base_num_query_heads: 32
base_num_kv_heads: 8
base_num_decoder_layers: 16
base_mlp_dim: 8192
head_dim: 64
mlp_activations: ["silu", "linear"]
vocab_size: 128256
enable_dropout: False
logits_via_embedding: True
normalization_layer_epsilon: 1.0e-5
rope_max_timescale: 500_000
decoder_block: "llama2" # Uses the same decoder block as llama2
attention: "dot_product"  # head_dim 64 is too small for splash/flash attention
